{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M6 EDA Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercises\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "  <li>Import the Hep Dataset and perform EDA as outlined below to the text column.</li>\n",
    "    \n",
    "    \n",
    "                Plot the top 10 highest frequency words.\n",
    "                Calculate the normalised frequency for top 10 most frequently occurring words.\n",
    "                Create a word cloud for the top 100 words (in terms of frequency)\n",
    "                Calculate the lexical diversity for the text\n",
    "                Show keyword in context for the top 10 words.\n",
    "                Generate collocations\n",
    "\n",
    "     \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries used\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.data.path.append(\"../pre_course/nltk_data\")\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hep = pd.read_pickle(\"../data/Hep_Dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to change text column to string\n",
    "def change_to_string(ptext):\n",
    "    return \" \".join(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hep['Text'] = hep['Text'].apply(change_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep a copy of the df \n",
    "hep2 = hep.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the top 10 highest frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some utility preprocessing functions for use later.\n",
    "\n",
    "def remove_punct(ptext):\n",
    "    \n",
    "    for each_punctuation_mark in string.punctuation:\n",
    "        ptext = ptext.replace(each_punctuation_mark, \"\")\n",
    "        \n",
    "    return ptext\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_stopwords(token):\n",
    "    return [item for item in token.split(\" \") if item not in stop_words and item != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lowercase\n",
    "hep[\"Text\"] = hep[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "hep['Text'] = hep['Text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "hep['Text_nostopwords'] = hep['Text'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.loc[5, 'Text_nostopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alltext = []\n",
    "for item in hep['Text_nostopwords']:\n",
    "    alltext.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=collections.Counter(alltext)\n",
    "print(dict(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_order = sorted(dict(counter).items(), key=lambda x: x[1], reverse=True)\n",
    "token=[]\n",
    "count =[]\n",
    "for i in sort_order:\n",
    "    token.append(i[0])\n",
    "    count.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotall(px, py):\n",
    "    \n",
    "    plt.xticks(fontsize=12, rotation=90)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.plot(px, py)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "plotall(token[:10], count[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the normalised frequency for top 10 most frequently occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get total tokens in text column\n",
    "total = sum(dict(counter).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(10):\n",
    "    print (token[num], \"-\", count[num]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a word cloud for the top 100 words (in terms of frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the text data\n",
    "text = \" \".join(hep['Text'])\n",
    "\n",
    "# The text string is then passed to the wordcloud function:\n",
    "wordcloud = WordCloud(max_font_size=50, \n",
    "                      max_words=100, \n",
    "                      background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the lexical diversity for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Lexical Diversity:',len(dict(counter).keys())/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show keyword in context for the top 10 words.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = text.split(\" \")\n",
    "textList = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList.concordance('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Given list of tokens, return collocations.\n",
    "    '''\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    bigramFinder.apply_word_filter(lambda word: len(word) < 3 or word.lower() in ignored_words)\n",
    "    \n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    \n",
    "    bigramFreqTable = (pd.DataFrame(list(bigram_freq), columns=['bigram','freq'])\n",
    "                       .sort_values(by='freq', ascending=False))\n",
    "   \n",
    "    return bigramFreqTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_collocations(textList).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
